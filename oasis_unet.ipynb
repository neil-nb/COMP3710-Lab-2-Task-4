{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as image\n",
    "\n",
    "import time\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA not found. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OasisDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom PyTorch dataset for loading OASIS images and labels.\n",
    "\n",
    "    Attributes (Where N - batch, C - channels, D - depth, H - height, W - width):\n",
    "        images (torch.Tensor): Image tensors of shape (N, C, H, W).\n",
    "        labels (torch.Tensor): Label tensors of shape (N, H, W, C) or (N, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        # Convert to torch tensors and ensure channels-first format\n",
    "        self.images = torch.from_numpy(images).permute(0, 3, 1, 2)  # (N, H, W, 1) â†’ (N, 1, H, W)\n",
    "        self.labels = torch.from_numpy(labels)  # (N, H, W, C) or (N, H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "    \n",
    "def load_training(path):\n",
    "    \"\"\"\n",
    "    Load training images from the specified directory.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        image_list.append(im)\n",
    "    print(\"train_X shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.float32)\n",
    "\n",
    "def process_training(data_set):\n",
    "    \"\"\"\n",
    "    Normalise and reshape training images.\n",
    "    \"\"\"\n",
    "    data_set = data_set.astype(np.float32)\n",
    "    if data_set.max() > 1.0:\n",
    "        data_set = data_set / 255.0\n",
    "    data_set = data_set[:, :, :, np.newaxis]  # (N, H, W, 1)\n",
    "    return data_set\n",
    "\n",
    "def load_labels(path):\n",
    "    \"\"\"\n",
    "    Load label masks and map pixel values to integer class IDs.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :][im == unique_value] = i\n",
    "        image_list.append(one_hot)\n",
    "    print(\"train_y shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.uint8)\n",
    "\n",
    "def process_labels(seg_data):\n",
    "    \"\"\"\n",
    "    Convert integer label masks into one-hot encoded format.\n",
    "    \"\"\"\n",
    "    onehot_Y = []\n",
    "    for n in range(seg_data.shape[0]):\n",
    "        im = seg_data[n]\n",
    "        n_classes = 4\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1], n_classes), dtype=np.uint8)\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :, i][im == unique_value] = 1\n",
    "        onehot_Y.append(one_hot)\n",
    "    onehot_Y = np.array(onehot_Y)\n",
    "    print(\"Labels shape:\", onehot_Y.shape)\n",
    "    return onehot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_train\"))\n",
    "train_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_train\"))\n",
    "train_dataset = OasisDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_test\"))\n",
    "test_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_test\"))\n",
    "test_dataset = OasisDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a409470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-layer convolutional block used in UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet with encoder channel sizes similar to the provided VAE (1 -> 32 -> 64 -> 128 -> 256 -> 512).\n",
    "    The decoder mirrors the encoder and uses transposed convolutions for upsampling.\n",
    "    Final layer outputs n_classes channels (categorical / one-hot style output before Softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes: int = 4, input_channels: int = 1, base_filters: int = 32):\n",
    "        super().__init__()\n",
    "        f = base_filters\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(input_channels, f)                            # 1 -> 32\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f, f*2))     # 32 -> 64\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*2, f*4))   # 64 -> 128\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*4, f*8))   # 128 -> 256\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*8, f*16))  # 256 -> 512\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(f*16, f*8, kernel_size=2, stride=2)   # 512 -> 256\n",
    "        self.dec1 = DoubleConv(f*16, f*8)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)    # 256 -> 128\n",
    "        self.dec2 = DoubleConv(f*8, f*4)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)    # 128 -> 64\n",
    "        self.dec3 = DoubleConv(f*4, f*2)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)      # 64 -> 32\n",
    "        self.dec4 = DoubleConv(f*2, f)\n",
    "\n",
    "        # Final conv to produce logits for each class\n",
    "        self.outc = nn.Conv2d(f, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)     # (N,   f,    H, W)\n",
    "        x2 = self.down1(x1)  # (N,  2f,  H/2, W/2)\n",
    "        x3 = self.down2(x2)  # (N,  4f,  H/4, W/4)\n",
    "        x4 = self.down3(x3)  # (N,  8f,  H/8, W/8)\n",
    "        x5 = self.down4(x4)  # (N, 16f, H/16, W/16)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.up1(x5)\n",
    "        d1 = torch.cat([d1, x4], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, x3], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat([d3, x2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat([d4, x1], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        logits = self.outc(d4)  # (N, n_classes, H, W)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52067263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_score(pred_softmax: torch.Tensor, target_onehot: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Dice score per class and return mean across classes.\n",
    "    pred_softmax : (N, C, H, W) probabilities after softmax\n",
    "    target_onehot: (N, H, W, C) or (N, C, H, W) one-hot target (expects float / 0/1)\n",
    "    Returns mean Dice across classes (averaged over batch and classes).\n",
    "    \"\"\"\n",
    "    if target_onehot.ndim == 4 and target_onehot.shape[1] == pred_softmax.shape[1]:\n",
    "        # (N, C, H, W)\n",
    "        target = target_onehot\n",
    "    elif target_onehot.ndim == 4 and target_onehot.shape[-1] == pred_softmax.shape[1]:\n",
    "        # (N, H, W, C) -> convert to (N, C, H, W)\n",
    "        target = target_onehot.permute(0, 3, 1, 2).float()\n",
    "    else:\n",
    "        raise ValueError(\"target_onehot shape incompatible with pred_softmax\")\n",
    "\n",
    "    # Flatten spatial dims\n",
    "    N, C, H, W = pred_softmax.shape\n",
    "    pred_flat = pred_softmax.contiguous().view(N, C, -1)\n",
    "    targ_flat = target.contiguous().view(N, C, -1)\n",
    "\n",
    "    # Compute intersection and unions per sample and per class\n",
    "    intersection = (pred_flat * targ_flat).sum(-1)  # (N, C)\n",
    "    cardinality = pred_flat.sum(-1) + targ_flat.sum(-1)  # (N, C)\n",
    "\n",
    "    dice = (2.0 * intersection + eps) / (cardinality + eps)  # (N, C)\n",
    "    return dice.mean()  # average over batch and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf58105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined cross entropy + Dice loss.\n",
    "    Uses class indices for CrossEntropy (converted from one-hot targets),\n",
    "    and soft Dice computed from softmax outputs and one-hot targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, dice_weight: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target_onehot: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns (loss, ce_loss_value, dice_loss_value)\n",
    "        logits: (N, C, H, W)\n",
    "        target_onehot: (N, H, W, C) or (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # Convert target to class indices for CrossEntropy\n",
    "        if target_onehot.ndim == 4 and target_onehot.shape[-1] == logits.shape[1]:\n",
    "            target_idx = target_onehot.argmax(axis=-1).long()  # (N, H, W)\n",
    "        elif target_onehot.ndim == 4 and target_onehot.shape[1] == logits.shape[1]:\n",
    "            target_idx = target_onehot.argmax(dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(\"target_onehot shape incompatible with logits\")\n",
    "\n",
    "        ce_loss = self.ce(logits, target_idx.to(logits.device))\n",
    "        probs = F.softmax(logits, dim=1)  # (N, C, H, W)\n",
    "\n",
    "        # Ensure one-hot in (N, C, H, W)\n",
    "        if target_onehot.shape[-1] == logits.shape[1]:\n",
    "            target_4ch = target_onehot.permute(0, 3, 1, 2).float().to(logits.device)\n",
    "        else:\n",
    "            target_4ch = target_onehot.float().to(logits.device)\n",
    "\n",
    "        dice_score = soft_dice_score(probs, target_4ch)\n",
    "        dice_loss = 1.0 - dice_score\n",
    "\n",
    "        loss = ce_loss + self.dice_weight * dice_loss\n",
    "        return loss, ce_loss.item(), dice_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3825a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, loader: DataLoader, n_classes: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Compute Dice per class (averaged over dataset) and mean Dice.\n",
    "    Returns (dice_per_class_list, mean_dice)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dices_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    counts = np.zeros(n_classes, dtype=np.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)  # (N, C, H, W)\n",
    "            logits = model(images)      # (N, n_classes, H, W)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = probs.argmax(dim=1)  # (N, H, W)\n",
    "\n",
    "            # Convert labels to class indices if they are one-hot (N, H, W, C)\n",
    "            if labels.ndim == 4 and labels.shape[-1] == logits.shape[1]:\n",
    "                targets = labels.argmax(axis=-1)\n",
    "            elif labels.ndim == 4 and labels.shape[1] == logits.shape[1]:\n",
    "                targets = labels.argmax(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"labels shape incompatible with logits\")\n",
    "\n",
    "            targets = targets.numpy()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "\n",
    "            # accumulate Dice per class\n",
    "            for c in range(n_classes):\n",
    "                # For each image in batch compute dice for class c\n",
    "                batch_dice = []\n",
    "                for b in range(targets.shape[0]):\n",
    "                    target_mask = (targets[b] == c).astype(np.uint8).ravel()\n",
    "                    pred_mask = (preds_np[b] == c).astype(np.uint8).ravel()\n",
    "\n",
    "                    if target_mask.sum() == 0 and pred_mask.sum() == 0:\n",
    "                        # edge case: no positive pixels in both -> define dice as 1 for this sample/class\n",
    "                        dice_val = 1.0\n",
    "                    elif target_mask.sum() == 0 and pred_mask.sum() > 0:\n",
    "                        dice_val = 0.0\n",
    "                    else:\n",
    "                        intersection = (target_mask & pred_mask).sum()\n",
    "                        dice_val = (2.0 * intersection) / (target_mask.sum() + pred_mask.sum() + 1e-6)\n",
    "\n",
    "                    batch_dice.append(dice_val)\n",
    "\n",
    "                dices_per_class[c] += np.sum(batch_dice)\n",
    "                counts[c] += len(batch_dice)\n",
    "\n",
    "    mean_per_class = (dices_per_class / (counts + 1e-12)).tolist()\n",
    "    mean_dice = float(np.mean(mean_per_class))\n",
    "    return mean_per_class, mean_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b79897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentations(model: nn.Module, loader: DataLoader, n_classes: int, device: torch.device,\n",
    "                            num_batches: int = 1, save_dir: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualise a few segmentation results: input, ground truth and predicted mask (class indices).\n",
    "    If save_dir is provided, images are saved to that directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True) if save_dir else None\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = probs.argmax(dim=1)  # (N, H, W)\n",
    "\n",
    "            # Convert labels to indices if one-hot\n",
    "            if labels.ndim == 4 and labels.shape[-1] == n_classes:\n",
    "                targets = labels.argmax(axis=-1)\n",
    "            elif labels.ndim == 4 and labels.shape[1] == n_classes:\n",
    "                targets = labels.argmax(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"labels shape incompatible with logits\")\n",
    "\n",
    "            images_np = images.cpu().numpy()  # (N, C, H, W)\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            targets_np = targets.numpy()\n",
    "\n",
    "            batch_size = images_np.shape[0]\n",
    "            for b in range(batch_size):\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                img = images_np[b].squeeze()\n",
    "                axes[0].imshow(img, cmap='gray')\n",
    "                axes[0].set_title(\"Input\")\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                axes[1].imshow(targets_np[b], cmap='tab20')\n",
    "                axes[1].set_title(\"Ground truth (class indices)\")\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                axes[2].imshow(preds_np[b], cmap='tab20')\n",
    "                axes[2].set_title(\"Prediction (class indices)\")\n",
    "                axes[2].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                if save_dir:\n",
    "                    plt.savefig(os.path.join(save_dir, f\"seg_{shown:04d}.png\"))\n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "\n",
    "                shown += 1\n",
    "                if shown >= num_batches * batch_size:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, n_classes: int, device: torch.device, \n",
    "               epochs: int = 100, lr: float = 1e-3, dice_weight: float = 1.0, target_mean_dice: Optional[float] = 0.90, checkpoint_path: str = \"unet_best.pth\"):\n",
    "    \"\"\"\n",
    "    Train UNet with combined CrossEntropy + Dice loss. Optionally early stop when target_mean_dice is reached on the val set.\n",
    "    Saves best checkpoint to checkpoint_path.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = CombinedLoss(dice_weight=dice_weight)\n",
    "\n",
    "    best_val_dice = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_ce = 0.0\n",
    "        running_dice = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            # labels may be (N, H, W, C) or (N, C, H, W)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss, ce_val, dice_val = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_ce += ce_val * batch_size\n",
    "            running_dice += dice_val * batch_size\n",
    "            n_samples += batch_size\n",
    "\n",
    "        train_loss = running_loss / n_samples\n",
    "        train_ce = running_ce / n_samples\n",
    "        train_dice_loss = running_dice / n_samples\n",
    "\n",
    "        # Validation\n",
    "        val_per_class, val_mean_dice = evaluate_model(model, val_loader, n_classes=n_classes, device=device)\n",
    "\n",
    "        # Save best\n",
    "        if val_mean_dice > best_val_dice:\n",
    "            best_val_dice = val_mean_dice\n",
    "            best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_mean_dice': best_val_dice,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch [{epoch}/{epochs}]  Time: {t1-t0:.1f}s  TrainLoss: {train_loss:.6f}  CE: {train_ce:.6f}  DiceLoss: {train_dice_loss:.6f}  ValMeanDice: {val_mean_dice:.4f}\")\n",
    "\n",
    "        # Early stopping if target reached\n",
    "        if (target_mean_dice is not None) and (val_mean_dice >= target_mean_dice):\n",
    "            print(f\"Target mean DSC {target_mean_dice:.3f} reached at epoch {epoch}. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best val mean DSC {best_val_dice:.4f} at epoch {best_epoch}\")\n",
    "    return model, best_val_dice, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81312ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
