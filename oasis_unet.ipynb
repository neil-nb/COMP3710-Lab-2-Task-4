{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd3ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as image\n",
    "\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af87dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA not found. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bfc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OasisDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom PyTorch dataset for loading OASIS images and labels.\n",
    "\n",
    "    Attributes (Where N - batch, C - channels, D - depth, H - height, W - width):\n",
    "        images (torch.Tensor): Image tensors of shape (N, C, H, W).\n",
    "        labels (torch.Tensor): Label tensors of shape (N, H, W, C) or (N, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        # Convert to torch tensors and ensure channels-first format\n",
    "        self.images = torch.from_numpy(images).permute(0, 3, 1, 2)  # (N, H, W, 1) â†’ (N, 1, H, W)\n",
    "        self.labels = torch.from_numpy(labels)  # (N, H, W, C) or (N, H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "    \n",
    "def load_training(path):\n",
    "    \"\"\"\n",
    "    Load training images from the specified directory.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        image_list.append(im)\n",
    "    print(\"train_X shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.float32)\n",
    "\n",
    "def process_training(data_set):\n",
    "    \"\"\"\n",
    "    Normalise and reshape training images.\n",
    "    \"\"\"\n",
    "    data_set = data_set.astype(np.float32)\n",
    "    if data_set.max() > 1.0:\n",
    "        data_set = data_set / 255.0\n",
    "    data_set = data_set[:, :, :, np.newaxis]  # (N, H, W, 1)\n",
    "    return data_set\n",
    "\n",
    "def load_labels(path):\n",
    "    \"\"\"\n",
    "    Load label masks and map pixel values to integer class IDs.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :][im == unique_value] = i\n",
    "        image_list.append(one_hot)\n",
    "    print(\"train_y shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.uint8)\n",
    "\n",
    "def process_labels(seg_data):\n",
    "    \"\"\"\n",
    "    Convert integer label masks into one-hot encoded format.\n",
    "    \"\"\"\n",
    "    onehot_Y = []\n",
    "    for n in range(seg_data.shape[0]):\n",
    "        im = seg_data[n]\n",
    "        n_classes = 4\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1], n_classes), dtype=np.uint8)\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :, i][im == unique_value] = 1\n",
    "        onehot_Y.append(one_hot)\n",
    "    onehot_Y = np.array(onehot_Y)\n",
    "    print(\"Labels shape:\", onehot_Y.shape)\n",
    "    return onehot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548ccda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (9664, 256, 256)\n",
      "train_y shape: (9664, 256, 256)\n",
      "Labels shape: (9664, 256, 256, 4)\n",
      "train_X shape: (544, 256, 256)\n",
      "train_y shape: (544, 256, 256)\n",
      "Labels shape: (544, 256, 256, 4)\n"
     ]
    }
   ],
   "source": [
    "train_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_train\"))\n",
    "train_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_train\"))\n",
    "train_dataset = OasisDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_test\"))\n",
    "test_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_test\"))\n",
    "test_dataset = OasisDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a409470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-layer convolutional block used in UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f3c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet with encoder channel sizes similar to the provided VAE (1 -> 32 -> 64 -> 128 -> 256 -> 512).\n",
    "    The decoder mirrors the encoder and uses transposed convolutions for upsampling.\n",
    "    Final layer outputs n_classes channels (categorical / one-hot style output before Softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes: int = 4, input_channels: int = 1, base_filters: int = 32):\n",
    "        super().__init__()\n",
    "        f = base_filters\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(input_channels, f)                            # 1 -> 32\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f, f*2))     # 32 -> 64\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*2, f*4))   # 64 -> 128\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*4, f*8))   # 128 -> 256\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*8, f*16))  # 256 -> 512\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(f*16, f*8, kernel_size=2, stride=2)   # 512 -> 256\n",
    "        self.dec1 = DoubleConv(f*16, f*8)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)    # 256 -> 128\n",
    "        self.dec2 = DoubleConv(f*8, f*4)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)    # 128 -> 64\n",
    "        self.dec3 = DoubleConv(f*4, f*2)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)      # 64 -> 32\n",
    "        self.dec4 = DoubleConv(f*2, f)\n",
    "\n",
    "        # Final conv to produce logits for each class\n",
    "        self.outc = nn.Conv2d(f, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)     # (N,   f,    H, W)\n",
    "        x2 = self.down1(x1)  # (N,  2f,  H/2, W/2)\n",
    "        x3 = self.down2(x2)  # (N,  4f,  H/4, W/4)\n",
    "        x4 = self.down3(x3)  # (N,  8f,  H/8, W/8)\n",
    "        x5 = self.down4(x4)  # (N, 16f, H/16, W/16)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.up1(x5)\n",
    "        d1 = torch.cat([d1, x4], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, x3], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat([d3, x2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat([d4, x1], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        logits = self.outc(d4)  # (N, n_classes, H, W)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52067263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_score(pred_softmax: torch.Tensor, target_onehot: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Dice score per class and return mean across classes.\n",
    "    pred_softmax : (N, C, H, W) probabilities after softmax\n",
    "    target_onehot: (N, H, W, C) or (N, C, H, W) one-hot target (expects float / 0/1)\n",
    "    Returns mean Dice across classes (averaged over batch and classes).\n",
    "    \"\"\"\n",
    "    if target_onehot.ndim == 4 and target_onehot.shape[1] == pred_softmax.shape[1]:\n",
    "        # (N, C, H, W)\n",
    "        target = target_onehot\n",
    "    elif target_onehot.ndim == 4 and target_onehot.shape[-1] == pred_softmax.shape[1]:\n",
    "        # (N, H, W, C) -> convert to (N, C, H, W)\n",
    "        target = target_onehot.permute(0, 3, 1, 2).float()\n",
    "    else:\n",
    "        raise ValueError(\"target_onehot shape incompatible with pred_softmax\")\n",
    "\n",
    "    # Flatten spatial dims\n",
    "    N, C, H, W = pred_softmax.shape\n",
    "    pred_flat = pred_softmax.contiguous().view(N, C, -1)\n",
    "    targ_flat = target.contiguous().view(N, C, -1)\n",
    "\n",
    "    # Compute intersection and unions per sample and per class\n",
    "    intersection = (pred_flat * targ_flat).sum(-1)  # (N, C)\n",
    "    cardinality = pred_flat.sum(-1) + targ_flat.sum(-1)  # (N, C)\n",
    "\n",
    "    dice = (2.0 * intersection + eps) / (cardinality + eps)  # (N, C)\n",
    "    return dice.mean()  # average over batch and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf58105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined cross entropy + Dice loss.\n",
    "    Uses class indices for CrossEntropy (converted from one-hot targets),\n",
    "    and soft Dice computed from softmax outputs and one-hot targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, dice_weight: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target_onehot: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns (loss, ce_loss_value, dice_loss_value)\n",
    "        logits: (N, C, H, W)\n",
    "        target_onehot: (N, H, W, C) or (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # Convert target to class indices for CrossEntropy\n",
    "        if target_onehot.ndim == 4 and target_onehot.shape[-1] == logits.shape[1]:\n",
    "            target_idx = target_onehot.argmax(axis=-1).long()  # (N, H, W)\n",
    "        elif target_onehot.ndim == 4 and target_onehot.shape[1] == logits.shape[1]:\n",
    "            target_idx = target_onehot.argmax(dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(\"target_onehot shape incompatible with logits\")\n",
    "\n",
    "        ce_loss = self.ce(logits, target_idx.to(logits.device))\n",
    "        probs = F.softmax(logits, dim=1)  # (N, C, H, W)\n",
    "\n",
    "        # Ensure one-hot in (N, C, H, W)\n",
    "        if target_onehot.shape[-1] == logits.shape[1]:\n",
    "            target_4ch = target_onehot.permute(0, 3, 1, 2).float().to(logits.device)\n",
    "        else:\n",
    "            target_4ch = target_onehot.float().to(logits.device)\n",
    "\n",
    "        dice_score = soft_dice_score(probs, target_4ch)\n",
    "        dice_loss = 1.0 - dice_score\n",
    "\n",
    "        loss = ce_loss + self.dice_weight * dice_loss\n",
    "        return loss, ce_loss.item(), dice_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3825a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, loader: DataLoader, n_classes: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Compute Dice per class (averaged over dataset) and mean Dice.\n",
    "    Returns (dice_per_class_list, mean_dice)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dices_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    counts = np.zeros(n_classes, dtype=np.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)  # (N, C, H, W)\n",
    "            logits = model(images)      # (N, n_classes, H, W)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = probs.argmax(dim=1)  # (N, H, W)\n",
    "\n",
    "            # Convert labels to class indices if they are one-hot (N, H, W, C)\n",
    "            if labels.ndim == 4 and labels.shape[-1] == logits.shape[1]:\n",
    "                targets = labels.argmax(axis=-1)\n",
    "            elif labels.ndim == 4 and labels.shape[1] == logits.shape[1]:\n",
    "                targets = labels.argmax(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"labels shape incompatible with logits\")\n",
    "\n",
    "            targets = targets.numpy()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "\n",
    "            # accumulate Dice per class\n",
    "            for c in range(n_classes):\n",
    "                # For each image in batch compute dice for class c\n",
    "                batch_dice = []\n",
    "                for b in range(targets.shape[0]):\n",
    "                    target_mask = (targets[b] == c).astype(np.uint8).ravel()\n",
    "                    pred_mask = (preds_np[b] == c).astype(np.uint8).ravel()\n",
    "\n",
    "                    if target_mask.sum() == 0 and pred_mask.sum() == 0:\n",
    "                        # edge case: no positive pixels in both -> define dice as 1 for this sample/class\n",
    "                        dice_val = 1.0\n",
    "                    elif target_mask.sum() == 0 and pred_mask.sum() > 0:\n",
    "                        dice_val = 0.0\n",
    "                    else:\n",
    "                        intersection = (target_mask & pred_mask).sum()\n",
    "                        dice_val = (2.0 * intersection) / (target_mask.sum() + pred_mask.sum() + 1e-6)\n",
    "\n",
    "                    batch_dice.append(dice_val)\n",
    "\n",
    "                dices_per_class[c] += np.sum(batch_dice)\n",
    "                counts[c] += len(batch_dice)\n",
    "\n",
    "    mean_per_class = np.round((dices_per_class / (counts + 1e-12)), 4).tolist()\n",
    "    mean_dice = float(np.mean(mean_per_class))\n",
    "    return mean_per_class, mean_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b79897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentations(model: nn.Module, loader: DataLoader, n_classes: int, device: torch.device,\n",
    "                            num_batches: int = 1, save_dir: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualise a few segmentation results: input, ground truth and predicted mask (class indices).\n",
    "    If save_dir is provided, images are saved to that directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True) if save_dir else None\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = probs.argmax(dim=1)  # (N, H, W)\n",
    "\n",
    "            # Convert labels to indices if one-hot\n",
    "            if labels.ndim == 4 and labels.shape[-1] == n_classes:\n",
    "                targets = labels.argmax(axis=-1)\n",
    "            elif labels.ndim == 4 and labels.shape[1] == n_classes:\n",
    "                targets = labels.argmax(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"labels shape incompatible with logits\")\n",
    "\n",
    "            images_np = images.cpu().numpy()  # (N, C, H, W)\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            targets_np = targets.numpy()\n",
    "\n",
    "            batch_size = images_np.shape[0]\n",
    "            for b in range(batch_size):\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                img = images_np[b].squeeze()\n",
    "                axes[0].imshow(img, cmap='gray')\n",
    "                axes[0].set_title(\"Input\")\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                axes[1].imshow(targets_np[b], cmap='tab20')\n",
    "                axes[1].set_title(\"Ground truth (class indices)\")\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                axes[2].imshow(preds_np[b], cmap='tab20')\n",
    "                axes[2].set_title(\"Prediction (class indices)\")\n",
    "                axes[2].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                if save_dir:\n",
    "                    plt.savefig(os.path.join(save_dir, f\"seg_{shown:04d}.png\"))\n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "\n",
    "                shown += 1\n",
    "                if shown >= num_batches * batch_size:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, n_classes: int, device: torch.device, \n",
    "               epochs: int = 100, lr: float = 1e-4, dice_weight: float = 1.0, target_mean_dice: Optional[float] = 0.90, checkpoint_path: Optional[str] = \"unet_best.pth\"):\n",
    "    \"\"\"\n",
    "    Train UNet with combined CrossEntropy + Dice loss. Optionally early stop when target_mean_dice is reached on the val set.\n",
    "    Optionally saves best checkpoint to checkpoint_path.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = CombinedLoss(dice_weight=dice_weight)\n",
    "\n",
    "    best_val_dice = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_ce = 0.0\n",
    "        running_dice = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            # labels may be (N, H, W, C) or (N, C, H, W)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss, ce_val, dice_val = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_ce += ce_val * batch_size\n",
    "            running_dice += dice_val * batch_size\n",
    "            n_samples += batch_size\n",
    "\n",
    "        train_loss = running_loss / n_samples\n",
    "        train_ce = running_ce / n_samples\n",
    "        train_dice_loss = running_dice / n_samples\n",
    "\n",
    "        # Validation\n",
    "        val_per_class, val_mean_dice = evaluate_model(model, val_loader, n_classes=n_classes, device=device)\n",
    "\n",
    "        # Save best\n",
    "        if val_mean_dice > best_val_dice:\n",
    "            best_val_dice = val_mean_dice\n",
    "            best_epoch = epoch\n",
    "            if checkpoint_path:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_mean_dice': best_val_dice,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch [{epoch}/{epochs}]  Time: {t1-t0:.1f}s  TrainLoss: {train_loss:.4f}  CE: {train_ce:.4f}  DiceLoss: {train_dice_loss:.4f}  ValMeanDice: {val_mean_dice:.4f}\")\n",
    "\n",
    "        # Early stopping if target reached\n",
    "        if (target_mean_dice is not None) and (val_mean_dice >= target_mean_dice):\n",
    "            print(f\"Target mean DSC {target_mean_dice:.4f} reached at epoch {epoch}. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best val mean DSC {best_val_dice:.4f} at epoch {best_epoch}\")\n",
    "    return model, best_val_dice, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81312ae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m test_loader\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m trained_model, best_dice, best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_unet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdice_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDICE_WEIGHT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mean_dice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_MEAN_DICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#\"unet_best.pth\"\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate final model on test set and print per-class dice\u001b[39;00m\n\u001b[0;32m     35\u001b[0m per_class_dice, mean_dice \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, val_loader, n_classes\u001b[38;5;241m=\u001b[39mN_CLASSES, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mtrain_unet\u001b[1;34m(model, train_loader, val_loader, n_classes, device, epochs, lr, dice_weight, target_mean_dice, checkpoint_path)\u001b[0m\n\u001b[0;32m     42\u001b[0m train_dice_loss \u001b[38;5;241m=\u001b[39m running_dice \u001b[38;5;241m/\u001b[39m n_samples\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m val_per_class, val_mean_dice \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Save best\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_mean_dice \u001b[38;5;241m>\u001b[39m best_val_dice:\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, loader, n_classes, device)\u001b[0m\n\u001b[0;32m     47\u001b[0m             dices_per_class[c] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(batch_dice)\n\u001b[0;32m     48\u001b[0m             counts[c] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_dice)\n\u001b[1;32m---> 50\u001b[0m mean_per_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((dices_per_class \u001b[38;5;241m/\u001b[39m (counts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m)), \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     51\u001b[0m mean_dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(mean_per_class))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_per_class, mean_dice\n",
      "\u001b[1;31mTypeError\u001b[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "# Hyperparameters you can tune\n",
    "N_CLASSES = 4\n",
    "INPUT_CHANNELS = 1\n",
    "BASE_FILTERS = 32\n",
    "LR = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "DICE_WEIGHT = 1.0\n",
    "TARGET_MEAN_DICE = 0.90  # early stop target if you want\n",
    "\n",
    "# Create model\n",
    "unet = UNet(n_classes=N_CLASSES, input_channels=INPUT_CHANNELS, base_filters=BASE_FILTERS).to(device)\n",
    "\n",
    "# If your train_loader/test_loader created earlier have different batch sizes or are named differently,\n",
    "# replace them appropriately below.\n",
    "# Here we reuse train_loader and test_loader from your earlier setup.\n",
    "# If you want a validation split, create a valid_loader. For simplicity we'll use test_loader as val_loader.\n",
    "val_loader = test_loader\n",
    "\n",
    "# Train\n",
    "trained_model, best_dice, best_epoch = train_unet(\n",
    "    unet,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_classes=N_CLASSES,\n",
    "    device=device,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    dice_weight=DICE_WEIGHT,\n",
    "    target_mean_dice=TARGET_MEAN_DICE,\n",
    "    checkpoint_path=None #\"unet_best.pth\"\n",
    ")\n",
    "\n",
    "# Evaluate final model on test set and print per-class dice\n",
    "per_class_dice, mean_dice = evaluate_model(trained_model, val_loader, n_classes=N_CLASSES, device=device)\n",
    "print(\"Per-class DSC:\", per_class_dice)\n",
    "print(\"Mean DSC:\", mean_dice)\n",
    "\n",
    "# Visualise sample segmentations\n",
    "visualize_segmentations(trained_model, val_loader, n_classes=N_CLASSES, device=device, num_batches=2) #, save_dir=\"seg_results\")\n",
    "\n",
    "# Optionally load best checkpoint later\n",
    "# ckpt = torch.load(\"unet_best.pth\", map_location=device)\n",
    "# unet.load_state_dict(ckpt['model_state_dict'])\n",
    "# print(\"Loaded checkpoint with val_mean_dice =\", ckpt['val_mean_dice'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
