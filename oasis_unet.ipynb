{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import imageio.v2 as image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA not found. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OasisDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom PyTorch dataset for loading OASIS images and labels.\n",
    "\n",
    "    Attributes (Where N - batch, C - channels, D - depth, H - height, W - width):\n",
    "        images (torch.Tensor): Image tensors of shape (N, C, H, W).\n",
    "        labels (torch.Tensor): Label tensors of shape (N, H, W, C) or (N, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        # Convert to torch tensors and ensure channels-first format\n",
    "        self.images = torch.from_numpy(images).permute(0, 3, 1, 2)  # (N, H, W, 1) â†’ (N, 1, H, W)\n",
    "        self.labels = torch.from_numpy(labels)  # (N, H, W, C) or (N, H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "    \n",
    "def load_training(path):\n",
    "    \"\"\"\n",
    "    Load training images from the specified directory.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        image_list.append(im)\n",
    "    print(\"train_X shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.float32)\n",
    "\n",
    "def process_training(data_set):\n",
    "    \"\"\"\n",
    "    Normalise and reshape training images.\n",
    "    \"\"\"\n",
    "    data_set = data_set.astype(np.float32)\n",
    "    if data_set.max() > 1.0:\n",
    "        data_set = data_set / 255.0\n",
    "    data_set = data_set[:, :, :, np.newaxis]  # (N, H, W, 1)\n",
    "    return data_set\n",
    "\n",
    "def load_labels(path):\n",
    "    \"\"\"\n",
    "    Load label masks and map pixel values to integer class IDs.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :][im == unique_value] = i\n",
    "        image_list.append(one_hot)\n",
    "    print(\"train_y shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.uint8)\n",
    "\n",
    "def process_labels(seg_data):\n",
    "    \"\"\"\n",
    "    Convert integer label masks into one-hot encoded format.\n",
    "    \"\"\"\n",
    "    onehot_Y = []\n",
    "    for n in range(seg_data.shape[0]):\n",
    "        im = seg_data[n]\n",
    "        n_classes = 4\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1], n_classes), dtype=np.uint8)\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :, i][im == unique_value] = 1\n",
    "        onehot_Y.append(one_hot)\n",
    "    onehot_Y = np.array(onehot_Y)\n",
    "    print(\"Labels shape:\", onehot_Y.shape)\n",
    "    return onehot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_train\"))\n",
    "train_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_train\"))\n",
    "train_dataset = OasisDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_test\"))\n",
    "test_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_test\"))\n",
    "test_dataset = OasisDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a409470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-layer convolutional block used in UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet with encoder channel sizes similar to the provided VAE (1 -> 32 -> 64 -> 128 -> 256 -> 512).\n",
    "    The decoder mirrors the encoder and uses transposed convolutions for upsampling.\n",
    "    Final layer outputs n_classes channels (categorical / one-hot style output before Softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes: int = 4, input_channels: int = 1, base_filters: int = 32):\n",
    "        super().__init__()\n",
    "        f = base_filters\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(input_channels, f)                            # 1 -> 32\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f, f*2))     # 32 -> 64\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*2, f*4))   # 64 -> 128\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*4, f*8))   # 128 -> 256\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*8, f*16))  # 256 -> 512\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(f*16, f*8, kernel_size=2, stride=2)   # 512 -> 256\n",
    "        self.dec1 = DoubleConv(f*16, f*8)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)    # 256 -> 128\n",
    "        self.dec2 = DoubleConv(f*8, f*4)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)    # 128 -> 64\n",
    "        self.dec3 = DoubleConv(f*4, f*2)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)      # 64 -> 32\n",
    "        self.dec4 = DoubleConv(f*2, f)\n",
    "\n",
    "        # Final conv to produce logits for each class\n",
    "        self.outc = nn.Conv2d(f, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)     # (N,   f,    H, W)\n",
    "        x2 = self.down1(x1)  # (N,  2f,  H/2, W/2)\n",
    "        x3 = self.down2(x2)  # (N,  4f,  H/4, W/4)\n",
    "        x4 = self.down3(x3)  # (N,  8f,  H/8, W/8)\n",
    "        x5 = self.down4(x4)  # (N, 16f, H/16, W/16)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.up1(x5)\n",
    "        d1 = torch.cat([d1, x4], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, x3], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat([d3, x2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat([d4, x1], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        logits = self.outc(d4)  # (N, n_classes, H, W)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52067263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_score(pred_softmax: torch.Tensor, target_onehot: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Dice score per class and return mean across classes.\n",
    "    pred_softmax : (N, C, H, W) probabilities after softmax\n",
    "    target_onehot: (N, H, W, C) or (N, C, H, W) one-hot target (expects float / 0/1)\n",
    "    Returns mean Dice across classes (averaged over batch and classes).\n",
    "    \"\"\"\n",
    "    if target_onehot.ndim == 4 and target_onehot.shape[1] == pred_softmax.shape[1]:\n",
    "        # (N, C, H, W)\n",
    "        target = target_onehot\n",
    "    elif target_onehot.ndim == 4 and target_onehot.shape[-1] == pred_softmax.shape[1]:\n",
    "        # (N, H, W, C) -> convert to (N, C, H, W)\n",
    "        target = target_onehot.permute(0, 3, 1, 2).float()\n",
    "    else:\n",
    "        raise ValueError(\"target_onehot shape incompatible with pred_softmax\")\n",
    "\n",
    "    # Flatten spatial dims\n",
    "    N, C, H, W = pred_softmax.shape\n",
    "    pred_flat = pred_softmax.contiguous().view(N, C, -1)\n",
    "    targ_flat = target.contiguous().view(N, C, -1)\n",
    "\n",
    "    # Compute intersection and unions per sample and per class\n",
    "    intersection = (pred_flat * targ_flat).sum(-1)  # (N, C)\n",
    "    cardinality = pred_flat.sum(-1) + targ_flat.sum(-1)  # (N, C)\n",
    "\n",
    "    dice = (2.0 * intersection + eps) / (cardinality + eps)  # (N, C)\n",
    "    return dice.mean()  # average over batch and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf58105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined cross entropy + Dice loss.\n",
    "    Uses class indices for CrossEntropy (converted from one-hot targets),\n",
    "    and soft Dice computed from softmax outputs and one-hot targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, dice_weight: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target_onehot: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns (loss, ce_loss_value, dice_loss_value)\n",
    "        logits: (N, C, H, W)\n",
    "        target_onehot: (N, H, W, C) or (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # Convert target to class indices for CrossEntropy\n",
    "        if target_onehot.ndim == 4 and target_onehot.shape[-1] == logits.shape[1]:\n",
    "            target_idx = target_onehot.argmax(axis=-1).long()  # (N, H, W)\n",
    "        elif target_onehot.ndim == 4 and target_onehot.shape[1] == logits.shape[1]:\n",
    "            target_idx = target_onehot.argmax(dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(\"target_onehot shape incompatible with logits\")\n",
    "\n",
    "        ce_loss = self.ce(logits, target_idx.to(logits.device))\n",
    "        probs = F.softmax(logits, dim=1)  # (N, C, H, W)\n",
    "\n",
    "        # Ensure one-hot in (N, C, H, W)\n",
    "        if target_onehot.shape[-1] == logits.shape[1]:\n",
    "            target_4ch = target_onehot.permute(0, 3, 1, 2).float().to(logits.device)\n",
    "        else:\n",
    "            target_4ch = target_onehot.float().to(logits.device)\n",
    "\n",
    "        dice_score = soft_dice_score(probs, target_4ch)\n",
    "        dice_loss = 1.0 - dice_score\n",
    "\n",
    "        loss = ce_loss + self.dice_weight * dice_loss\n",
    "        return loss, ce_loss.item(), dice_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3825a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, loader: DataLoader, n_classes: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Compute Dice per class (averaged over dataset) and mean Dice.\n",
    "    Returns (dice_per_class_list, mean_dice)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dices_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    counts = np.zeros(n_classes, dtype=np.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)  # (N, C, H, W)\n",
    "            logits = model(images)      # (N, n_classes, H, W)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = probs.argmax(dim=1)  # (N, H, W)\n",
    "\n",
    "            # Convert labels to class indices if they are one-hot (N, H, W, C)\n",
    "            if labels.ndim == 4 and labels.shape[-1] == logits.shape[1]:\n",
    "                targets = labels.argmax(axis=-1)\n",
    "            elif labels.ndim == 4 and labels.shape[1] == logits.shape[1]:\n",
    "                targets = labels.argmax(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"labels shape incompatible with logits\")\n",
    "\n",
    "            targets = targets.numpy()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "\n",
    "            # accumulate Dice per class\n",
    "            for c in range(n_classes):\n",
    "                # For each image in batch compute dice for class c\n",
    "                batch_dice = []\n",
    "                for b in range(targets.shape[0]):\n",
    "                    target_mask = (targets[b] == c).astype(np.uint8).ravel()\n",
    "                    pred_mask = (preds_np[b] == c).astype(np.uint8).ravel()\n",
    "\n",
    "                    if target_mask.sum() == 0 and pred_mask.sum() == 0:\n",
    "                        # edge case: no positive pixels in both -> define dice as 1 for this sample/class\n",
    "                        dice_val = 1.0\n",
    "                    elif target_mask.sum() == 0 and pred_mask.sum() > 0:\n",
    "                        dice_val = 0.0\n",
    "                    else:\n",
    "                        intersection = (target_mask & pred_mask).sum()\n",
    "                        dice_val = (2.0 * intersection) / (target_mask.sum() + pred_mask.sum() + 1e-6)\n",
    "\n",
    "                    batch_dice.append(dice_val)\n",
    "\n",
    "                dices_per_class[c] += np.sum(batch_dice)\n",
    "                counts[c] += len(batch_dice)\n",
    "\n",
    "    mean_per_class = (dices_per_class / (counts + 1e-12)).tolist()\n",
    "    mean_dice = float(np.mean(mean_per_class))\n",
    "    return mean_per_class, mean_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b79897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a976d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81312ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
