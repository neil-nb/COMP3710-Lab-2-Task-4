{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import imageio.v2 as image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA not found. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OasisDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom PyTorch dataset for loading OASIS images and labels.\n",
    "\n",
    "    Attributes (Where N - batch, C - channels, D - depth, H - height, W - width):\n",
    "        images (torch.Tensor): Image tensors of shape (N, C, H, W).\n",
    "        labels (torch.Tensor): Label tensors of shape (N, H, W, C) or (N, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        # Convert to torch tensors and ensure channels-first format\n",
    "        self.images = torch.from_numpy(images).permute(0, 3, 1, 2)  # (N, H, W, 1) â†’ (N, 1, H, W)\n",
    "        self.labels = torch.from_numpy(labels)  # (N, H, W, C) or (N, H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "    \n",
    "def load_training(path):\n",
    "    \"\"\"\n",
    "    Load training images from the specified directory.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        image_list.append(im)\n",
    "    print(\"train_X shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.float32)\n",
    "\n",
    "def process_training(data_set):\n",
    "    \"\"\"\n",
    "    Normalise and reshape training images.\n",
    "    \"\"\"\n",
    "    data_set = data_set.astype(np.float32)\n",
    "    if data_set.max() > 1.0:\n",
    "        data_set = data_set / 255.0\n",
    "    data_set = data_set[:, :, :, np.newaxis]  # (N, H, W, 1)\n",
    "    return data_set\n",
    "\n",
    "def load_labels(path):\n",
    "    \"\"\"\n",
    "    Load label masks and map pixel values to integer class IDs.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path + \"/*.png\"):\n",
    "        im = image.imread(filename)\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :][im == unique_value] = i\n",
    "        image_list.append(one_hot)\n",
    "    print(\"train_y shape:\", np.array(image_list).shape)\n",
    "    return np.array(image_list, dtype=np.uint8)\n",
    "\n",
    "def process_labels(seg_data):\n",
    "    \"\"\"\n",
    "    Convert integer label masks into one-hot encoded format.\n",
    "    \"\"\"\n",
    "    onehot_Y = []\n",
    "    for n in range(seg_data.shape[0]):\n",
    "        im = seg_data[n]\n",
    "        n_classes = 4\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1], n_classes), dtype=np.uint8)\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "            one_hot[:, :, i][im == unique_value] = 1\n",
    "        onehot_Y.append(one_hot)\n",
    "    onehot_Y = np.array(onehot_Y)\n",
    "    print(\"Labels shape:\", onehot_Y.shape)\n",
    "    return onehot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_train\"))\n",
    "train_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_train\"))\n",
    "train_dataset = OasisDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_X = process_training(load_training(\"oasis/keras_png_slices_data/keras_png_slices_test\"))\n",
    "test_Y = process_labels(load_labels(\"oasis/keras_png_slices_data/keras_png_slices_seg_test\"))\n",
    "test_dataset = OasisDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a409470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-layer convolutional block used in UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet with encoder channel sizes similar to the provided VAE (1 -> 32 -> 64 -> 128 -> 256 -> 512).\n",
    "    The decoder mirrors the encoder and uses transposed convolutions for upsampling.\n",
    "    Final layer outputs n_classes channels (categorical / one-hot style output before Softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes: int = 4, input_channels: int = 1, base_filters: int = 32):\n",
    "        super().__init__()\n",
    "        f = base_filters\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(input_channels, f)                            # 1 -> 32\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f, f*2))     # 32 -> 64\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*2, f*4))   # 64 -> 128\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*4, f*8))   # 128 -> 256\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(f*8, f*16))  # 256 -> 512\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(f*16, f*8, kernel_size=2, stride=2)   # 512 -> 256\n",
    "        self.dec1 = DoubleConv(f*16, f*8)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)    # 256 -> 128\n",
    "        self.dec2 = DoubleConv(f*8, f*4)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)    # 128 -> 64\n",
    "        self.dec3 = DoubleConv(f*4, f*2)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)      # 64 -> 32\n",
    "        self.dec4 = DoubleConv(f*2, f)\n",
    "\n",
    "        # Final conv to produce logits for each class\n",
    "        self.outc = nn.Conv2d(f, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)     # (N,   f,    H, W)\n",
    "        x2 = self.down1(x1)  # (N,  2f,  H/2, W/2)\n",
    "        x3 = self.down2(x2)  # (N,  4f,  H/4, W/4)\n",
    "        x4 = self.down3(x3)  # (N,  8f,  H/8, W/8)\n",
    "        x5 = self.down4(x4)  # (N, 16f, H/16, W/16)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.up1(x5)\n",
    "        d1 = torch.cat([d1, x4], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, x3], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat([d3, x2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat([d4, x1], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        logits = self.outc(d4)  # (N, n_classes, H, W)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52067263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf58105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3825a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b79897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a976d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81312ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
